# Financial Doc QA Assistant

A user-friendly Streamlit application that enables question answering on financial documents such as PDFs and Excel files using a local large language model served by Ollama.

<img width="918" height="347" alt="{A3297740-DBFD-4C6A-BC35-114950A50541}" src="https://github.com/user-attachments/assets/b34b9e52-9f67-4c83-8280-2474b76fc54c" />

---

## Overview

This project allows users to upload financial documents and ask questions about their content. It leverages the Ollama local LLM server with the powerful LLaMA v2 model to provide accurate answers in real-time, without the need for cloud services.

<img width="894" height="803" alt="{F7831374-E3D8-4C98-A5E2-5BEAC5B9ADEF}" src="https://github.com/user-attachments/assets/6a43f05d-5f5d-4617-86c0-7d88dc946cbd" />
<img width="894" height="596" alt="{4F8B0E3C-8F23-4845-A5CC-FD0C32247036}" src="https://github.com/user-attachments/assets/0f4a74eb-4f14-47df-8e9e-9077d7f5ab93" />

---

## Features

- Upload PDF or Excel files containing financial data and reports.
- Extract text and tables from uploaded documents automatically.
- Ask detailed questions about the uploaded financial documents.
- Local processing using Ollama, ensuring data privacy and low latency.
- Interactive web interface built with Streamlit for accessibility.

---

## Installation & Setup

### Prerequisites

- Python 3.8 or higher
- Ollama installed and running locally (`ollama serve` on port 11434)
- An Ollama-compatible model installed (e.g., `llama2:latest`)

### Steps

1. Clone this repository:
git clone [https://github.com/<your-username>/Financial-Doc-QA-Assistant.git](https://github.com/Sharmila-D3/Financial-Doc-QA-Assistant.git)
cd Financial-Doc-QA-Assistant

text

2. Create and activate a virtual environment:

python -m venv venv
source venv/bin/activate # On Windows: .\venv\Scripts\activate

text

3. Install dependencies:

pip install -r requirements.txt

text

4. Start the Ollama server (if not running):

ollama serve

text

5. Run the Streamlit app:

streamlit run main.py

text

---

## Usage

- Open `http://localhost:8501` in your browser.
- Upload a financial PDF or Excel file.
- Enter questions related to the document content in the input box.
- View answers generated by the local language model.

---

## Limitations

- Large document inputs may require trimming to fit model context length.
- Requires sufficient local system memory (>4GB recommended) for optimal model performance.
- Currently supports only supported document formats: PDF and Excel.

---

## Contributing

Contributions and feedback are welcome! Feel free to open issues or submit pull requests to improve the app.

---

## License

This project is licensed under the MIT License.

---

## Author

Your Name â€” [GitHub Profile](https://github.com/your-username)

---

## Acknowledgments

- Ollama for local LLM serving technology.
- LLaMA v2 model for powerful language understanding.
- Streamlit for quick and easy UI building.

---

